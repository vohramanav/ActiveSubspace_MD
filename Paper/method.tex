\section{Methodology}
\label{sec:method}

In this section, we provide details pertaining to the computational framework used to identify
the active subspace for the present application involving NEMD simulations of bulk transport
in Si. As discussed earlier, the process involves repeated evaluations of the gradient of the model output
i.e. the bulk thermal conductivity with respect to the uncertain SW parameters at random samples
in the input domain. If a perturbation method such as finite difference is used, then gradient
estimation at $N$ samples would require $N(d+1)$ model evaluations, where $d$ is the number
of uncertain inputs. Clearly, for a large $N$, this approach would quickly become prohibitive
considering that NEMD simulations are computationally intensive. Therefore, to overcome the
computational hurdle, we rely on estimating the gradient using a regression-based approach,
whereby, a linear regression fit is performed to the available set of model evaluations. Additionally, in order
to avoid unnecessary model evaluations, the computations are performed
in an iterative manner. Specific details pertaining to gradient estimation as well as active
subspace computation are discussed further below in~\ref{sub:gradient} and~\ref{sub:subspace}. The
active subspace can be exploited to perform a GSA of the uncertain SW potential parameters as discussed
later in~\ref{sub:scores}.

\subsection{Gradient estimation}
\label{sub:gradient} 

For a given set of model evaluations at $N$ random samples in the full space input domain,
we outline the procedure for estimating the gradient of the model output, $G$, and hence the
matrix, $\hat{\mat{C}}$ in~\eqref{eq:chat}.
Note that the samples are generated
according to the joint probability distribution of the canonical random variables
($\vec\xi$), $\pi_\vec\xi$. An independent set of $M$ samples is also generated from $\pi_\vec\xi$
using Monte Carlo sampling. However, model evaluations are not required at these $M$ samples.
For each sample in $M$, a least-squares fit to $s$ nearest neighbors in
$n_0$ is performed. The value of $s$ ranges from 2 to $N-1$; $s$ = $N$ is regarded as the global approximation.
In this work, we specify $s$ as $N-1$ to capture the gradients in a leave-one-out fashion.
The slope vector ($\vec{d}_i$) of the linear regression-fit is estimated and recorded.
The collection of $\vec{d}_i$'s thus obtained are used to estimate the matrix, $\hat{\mat{C}}$. The specific 
sequence of steps is outlined in Algorithm~\ref{alg:lla}.
%
\bigskip
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Constructing the matrix, $\hat{\mat{C}}$ in~\eqref{eq:chat} for a given set of $N$ model evaluations}
  \begin{algorithmic}[1]
	\Procedure{Regression-based Approach}{} 
	\State Draw $N$ random samples, $\{\bm{\xi}_j\}_{j=1}^{N}$ 
	according to $\pi_{\bm{\xi}}$.
	\State Compute $G(\bm\xi_j)$, $j=1, \ldots, N$.
	\State Draw $M$ random samples, $\{\bm{\zeta}_i\}_{i=1}^{M}$
	according to $\pi_{\bm{\xi}}$.
	\State Choose an integer $s \in [2,N-1]$ 
	\State For each $i=1, \ldots, M$, compute 
	\[
	\begin{aligned}
	\Phi_i &= \{ s \text{ nearest points in } \{\bm{\xi}_j\}_{j=1}^{N} \text{ to } \bm{\zeta}_i\}\\
	\vspace{-2mm}
	\Psi_i &= \text{subset of } G(\bm\xi_j) \text{ corresponding to the points in } \Phi_i\\
	\vspace{-2mm}
	 &{\color{white}=} \hspace{-7mm} \text{Least-squares fit:~} 
	 G(\bm\xi_j) \approx c_i + \vec{d}_i^T\bm{\xi}_j,  \bm{\xi}_j \in \Phi_i, G(\bm\xi_j) \in \Psi_i\\
	 \vspace{-2mm}
	  &{\color{white}=} \hspace{-7mm}\text{Record the slope vector,}~\vec{d}_i
	\end{aligned}
	\]
	\State Compute the matrix, $\hat{\mat{C}}$:
	\[
	\hat{\mat{C}} \approx \frac{1}{M} \sum_{i=1}^{M} \vec{d}_i\vec{d}_i^T 
	\]
	\EndProcedure
  \end{algorithmic}
  \label{alg:lla}
\end{breakablealgorithm}
\bigskip
%

\subsection{Subspace computation}
\label{sub:subspace} 

An initial set of $n_0$ samples is drawn according to the joint probability distribution
$\pi_{\vec{\xi}}$ of the inputs, $\vec{\xi}$. Model evaluations are performed at these
$n_0$ samples. 
An initial estimate of the symmetric positive semidefinite matrix, $\hat{\mat{C}}$ 
is then obtained using Algorithm~\ref{alg:lla}. The eigenvalue decomposition of 
$\hat{\mat{C}}$ is performed to obtain initial estimates of the eigenvectors and
corresponding eigenvalues:
%
\be
\hat{\mat{C}} = \frac{1}{M}\sum\limits_{i=1}^{M}\bm{d}_i\bm{d}_i^\top = \hat{\bm{W}}\hat{\bm{\Lambda}}\hat{\bm{W}}^\top.
\ee
%
At each subsequent iteration, a new set of Monte Carlo samples is generated using the joint 
probability law, $\pi_{\vec{\xi}}$. Since model evaluations are expensive, the number of new samples
is a factor of the initial number, $n_0$. The augmented set of available and newly generated model
evaluations are used to obtain improved estimates of $\hat{\mat{C}}$ using Algorithm~\ref{alg:lla}.
The eigenvalue decomposition of $\hat{\mat{C}}$ yields refined estimates of the eigenspace.
The improved eigenspace is partitioned as discussed earlier in~\ref{sub:as}
 to obtain the active subspace. For a given eigenvector ($j^{th}$) in the active subspace, 
 we compute the relative L-2 
 norm of the difference in squared values of corresponding components ($\varepsilon_j^k$) of the same eigenvector,
 computed during successive iterations (iterations $k$ and $k-1$) as follows:
%
\be
\varepsilon_j^k = \frac{\|(\hat{\mat{W}}_{1,j}^{k})^2 - 
                       (\hat{\mat{W}}_{1,j}^{k-1})^2\|_2}{\|(\hat{\mat{W}}_{1,j}^{k-1})^2\|_2}, 
                       j = 1,\ldots,p,
\label{eq:conv}
\ee
%
where $p$ denotes the dimension of the column space of $\mat{W}_1$ or the number of eigenvectors in
the active subspace.
The quantity, $\varepsilon_j^k$ is recorded as the $j^{th}$ component of $\vec\varepsilon^k$. Convergence is
established once the component with the highest magnitude is below a given tolerance, $\tau$. 
The sequence of steps for computing the active subspace is outlined
in Algorithm~\ref{alg:free}.
%
\bigskip
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{An iterative gradient-based approach for discovering the active subspace}
  \begin{algorithmic}[1]
\Require $\tau$
\Ensure $\hat{\mat{\Lambda}}$, $\hat{\mat{W}}$, $\bm{\nu}_p(G)$ %$\eta$. %
    \Procedure{Gradient-free}{}
    \State Set $k$ = 0
	\State Draw $n_k$ random samples, $\{\bm{\xi}_i\}_{i=1}^{n_k}$ 
         according to $\pi_{\bm{\xi}}$. 
    \State Set $N_\text{total}$ = $n_k$ 
	\State For each $i=1, \ldots, N_\text{total}$, compute
		\[
		\begin{aligned}
	          &{\color{white}=} G(\bm{\xi}_i)~\text{and}~\bm{g}^i = \nabla_{\bm{\xi}}G(\bm{\xi}_i)~
		 \text{using~Algorithm~\ref{alg:lla}}
		\end{aligned}
		\]
	\State Compute $\hat{\mat{C}}$ and its eigenvalue decomposition:
		\[
		\begin{aligned} 
		  \hat{\mat{C}} &= \frac{1}{N_\text{total}}\sum\limits_{i=1}^{N_\text{total}}[\bm{g}^i][\bm{g}^i]^\top 
		  &= \hat{\mat{W}}^{(k)}\hat{\mat{\Lambda}}^{(k)} \hat{\mat{W}}^{(k)\top}
		\end{aligned}
		\]
	\State Partition the eigenpairs:
	\[
		\begin{aligned}
		 \hat{\mat{\Lambda}}^{(k)}=
        	\begin{bmatrix} \hat{\mat{\Lambda}}_1^{(k)} & \\ & \hat{\mat{\Lambda}}_2^{(k)} \end{bmatrix}, 
        	\hat{\mat{W}}^{(k)}=\begin{bmatrix} \hat{\mat{W}}_1^{(k)} & \hat{\mat{W}}_2^{(k)} \end{bmatrix}, 
        	\hat{\mat{\Lambda}}_1^{(k)}\in \mathbb{R}^{N_p\times p}
		\end{aligned}
	\]
	\Loop
		\State Set $k$ = $k$ + 1
		\State Draw $n_k =  \lceil\beta n_{k-1}\rceil$  new samples 
                $\{\bm{\xi}_i\}_{i=1}^{n_k}$  $\beta\in[0,1]$
                
%	%	\State Project $\bm{\xi}_k$~$\rightarrow$~$\bm{\theta}_k$.%
		\State Set $N_\text{total}$ = $N_\text{total}$ + $n_k$ 
		\State Compute $\bm{g}^i = \nabla_{\bm{\xi}_i}G(\bm{\xi}_i)$, 
             	$i=n_{k-1}+1, \ldots, n_{k-1}+n_k$ using Algorithm~\ref{alg:lla} 
		\State Compute $\hat{\mat{C}}$ and its eigenvalue decomposition (see Step 6)
		\State Partition the eigenspace of $\hat{\mat{C}}$ as shown in Step 7 
		\State Compute, $\vec\varepsilon^k$ using~\eqref{eq:conv} 
		\If {$\max\left(\vec\varepsilon^k\right)<\tau$}
			\State break
		\EndIf
       \EndLoop
%	\State Compute the normalized activity scores, $\tilde{\nu}_{i,p}(G)$ using~\eqref{eq:ac} and~\eqref{eq:nac}
    \EndProcedure
  \end{algorithmic}
  \label{alg:free}
\end{breakablealgorithm}
\bigskip
%

\subsection{GSA using the Active Subspace}
\label{sub:scores} 

The resulting active subspace could be used to perform a GSA of the uncertain inputs i.e. the SW potential
parameters in two ways: (1) By estimating the total Sobol index for each SW parameter, and (2) By computing
the so-called activity scores. Both approaches are expected to yield consistent trends for the parametric 
sensitivities, and are discussed as follows. 

\subsubsection{Surrogate-based GSA}
\label{subsub:gsa_surr}

Consider a model output, $G= G(\xi_1,\xi_2,\ldots,\xi_N)$, where $\xi_i$'s are statistically independent uncertain
inputs to the model. Total Sobol index, $T_i(G)$, is a commonly used variance-based measure for global sensitivity 
analysis (GSA). For a given uncertain input, it accounts for its individual contribution as well as contribution due to
its interaction with other uncertain inputs, to the variability in $G$. Mathematically, it is expressed as follows:
%
\be
T_i(G) = 1 - 
\frac{\V[\mathbb{E}(G|\vec{\xi}_{\sim i})]}{\V(G)},
\label{eq:total}
\ee
%
where $\vec{\xi}_{\sim i}$ is a vector of uncertain inputs with the  $i^\text{th}$ entry removed, and $\V$ denotes the 
variance. The RHS in~\eqref{eq:total} involves multidimensional integrals and is typically estimated using numerical
techniques such as Monte Carlo integration. Hence, obtaining converged estimates of $T_i(G)$ might require a large 
number of model runs depending upon the nature of the dependence of $G$ on $\xi_i$'s. To overcome this challenge,
we exploit the surrogate, $\tilde{\mathcal{Y}}$ (see Algorithm~\ref{alg:surr} and related discussion in~\ref{sub:as})
that essentially maps the canonical random variables, $\vec{\xi}$ to the QoI i.e the bulk thermal conductivity as 
$\tilde{\mathcal{Y}}(\mat{W}_1^\top\vec{\xi})$. Note that $\vec{\xi}$ exhibits a linear
relationship with the physical parameters, $\vec{\theta}$ as discussed earlier in~\ref{sub:as}.
Thus, a large number of
independent random samples in the physical input domain can be mapped to the QoI to compute $T_i(G)$
with negligible effort. 

\subsubsection{Activity Scores for GSA}
\label{subsub:gsa_as}
Several recent efforts have
focused on an efficient computation of the first and total order Sobol indices
using Polynomial chaos surrogates~\cite{Sudret:2008}, density-based sensitivity measures~\cite{Plischke:2013},
 randomized orthogonal arrays~\cite{Tissot:2015}, and direct computation from the input-output 
 samples~\cite{Li:2016}. Additionally,
the derivative-based global sensitivity measures (DGSMs)~\cite{Sobol:2009, Lamboni:2013}
have been developed to estimate upper bounds on the total Sobol indices using a fraction of computational effort
otherwise required to estimate the indices themselves~\cite{Vohra:2018b}. It was shown 
in~\cite{Diaz:2016,Constantine:2017} (and later generalized for a broad range of input probability distributions 
in~\cite{Vohra:2018c}) that the eigenspace that constitutes the active subspace can be used to
approximate the DGSMs by evaluating the so-called activity scores. The activity score, $\nu_i$ for the 
$i^{th}$ uncertain input can be computed using the following expression:
%
\be
\nu_{i,p}(G) = \sum\limits_{j=1}^{p} \lambda_j w_{i,j}^2, i=1,\ldots,\Nt,
\label{eq:ac}
\ee
%
where $p$ denotes the dimensionality of the eigenspace. 
A more robust measure is the normalized activity score ($\tilde{\nu}_{i,p}(G)$) that ranges from 0 to 1:
%
\be
\tilde{\nu}_{i,p}(G) = \scalebox{1.25}{$\frac{\nu_{i,p}(G)}{\sum_i\nu_{i,p}(G)}$}.
\label{eq:nac}
\ee
%
The normalized activity scores are computed for the SW parameters to determine their relative contributions
to bulk thermal conductivity estimates of Si and compared with corresponding estimates using the surrogate-based
approach, in section~\ref{sec:results}. 


