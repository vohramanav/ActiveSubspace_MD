\section{Proposed Methodology}
\label{sec:method}

In this section, we present the computational framework for forward and inverse UQ, based on identifying
the active subspace in NEMD simulations of bulk transport
in Si. As discussed earlier, the process involves repeated evaluations of the gradient of the model output
i.e. the bulk thermal conductivity with respect to the uncertain SW parameters at random samples
in the input domain. If a perturbation method such as finite difference is used, then gradient
estimation at $N$ samples would require $N(d+1)$ model evaluations, where $d$ is the number
of uncertain inputs. Clearly, for a large $N$, this approach would quickly become prohibitive
considering that NEMD simulations are computationally intensive. Therefore, to overcome the
computational hurdle, we rely on estimating the gradient using a regression-based approach,
whereby, a linear regression fit is performed to the available set of model evaluations. Additionally, in order
to avoid unnecessary model evaluations, the computations are performed
in an iterative manner. Specific details pertaining to gradient estimation as well as active
subspace computation are discussed further below in~\ref{sub:gradient} and~\ref{sub:subspace}. 

A surrogate model is built in the low-dimensional active subspace, which provides several benefits with respect
to the computational effort: (1) the number of atomistic simulations used to construct the surrogate are
reduced; (2) the number of samples required for forward uncertainty propagation are reduced; (3) the number samples required for GSA to assess relative contributions of the uncertain parameters to the uncertainty in
bulk thermal conductivity are reduced; and (4) the computational effort pertaining to
Bayesian calibration is reduced.
Note that (1) and (2) are discussed in~\ref{sub:surr_sub}; (3) is discussed
in~\ref{sub:scores}; and (4) is discussed in~\ref{sub:ba_method}.

\subsection{Gradient estimation}
\label{sub:gradient} 

For a given set of model evaluations at $N$ random samples in the full space input domain,
we outline the procedure for estimating the gradient of the model output, $G$, and hence the
matrix, $\hat{\mat{C}}$ in~\eqref{eq:chat}.
Note that the samples are generated
according to the joint probability distribution of the canonical random variables
($\vec\xi$), $\pi_\vec\xi$. An independent set of $M$ samples is also generated from $\pi_\vec\xi$
using Monte Carlo sampling. However, model evaluations are not required at these $M$ samples.
For each sample in $M$, a least-squares fit to $s$ nearest neighbors in
$n_0$ is performed. The value of $s$ ranges from 2 to $N-1$; $s$ = $N$ is regarded as the global approximation.
In this work, we specify $s$ as $N-1$ to capture the gradients in a leave-one-out fashion.
The slope vector ($\vec{d}_i$) of the linear regression-fit is estimated and recorded.
The collection of $\vec{d}_i$'s thus obtained are used to estimate the matrix, $\hat{\mat{C}}$. The specific 
sequence of steps is outlined in Algorithm~\ref{alg:lla}.
%
\bigskip
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Constructing the matrix, $\hat{\mat{C}}$ in~\eqref{eq:chat} for a given set of $N$ model evaluations}
  \begin{algorithmic}[1]
	\Procedure{Regression-based Approach}{} 
	\State Draw $N$ random samples, $\{\bm{\xi}_j\}_{j=1}^{N}$ 
	according to $\pi_{\bm{\xi}}$.
	\State Compute $G(\bm\xi_j)$, $j=1, \ldots, N$.
	\State Draw $M$ random samples, $\{\bm{\zeta}_i\}_{i=1}^{M}$
	according to $\pi_{\bm{\xi}}$.
	\State Choose an integer $s \in [2,N-1]$ 
	\State For each $i=1, \ldots, M$, compute 
	\[
	\begin{aligned}
	\Phi_i &= \{ s \text{ nearest points in } \{\bm{\xi}_j\}_{j=1}^{N} \text{ to } \bm{\zeta}_i\}\\
	\vspace{-2mm}
	\Psi_i &= \text{subset of } G(\bm\xi_j) \text{ corresponding to the points in } \Phi_i\\
	\vspace{-2mm}
	 &{\color{white}=} \hspace{-7mm} \text{Least-squares fit:~} 
	 G(\bm\xi_j) \approx c_i + \vec{d}_i^T\bm{\xi}_j,  \bm{\xi}_j \in \Phi_i, G(\bm\xi_j) \in \Psi_i\\
	 \vspace{-2mm}
	  &{\color{white}=} \hspace{-7mm}\text{Record the slope vector,}~\vec{d}_i
	\end{aligned}
	\]
	\State Compute the matrix, $\hat{\mat{C}}$:
	\[
	\hat{\mat{C}} \approx \frac{1}{M} \sum_{i=1}^{M} \vec{d}_i\vec{d}_i^T 
	\]
	\EndProcedure
  \end{algorithmic}
  \label{alg:lla}
\end{breakablealgorithm}
\bigskip
%

\subsection{Subspace computation}
\label{sub:subspace} 

An initial set of $n_0$ samples is drawn according to the joint probability distribution
$\pi_{\vec{\xi}}$ of the inputs, $\vec{\xi}$. Model evaluations are performed at these
$n_0$ samples. 
An initial estimate of the symmetric positive semidefinite matrix, $\hat{\mat{C}}$ 
is then obtained using Algorithm~\ref{alg:lla}. The eigenvalue decomposition of 
$\hat{\mat{C}}$ is performed to obtain initial estimates of the eigenvectors and
corresponding eigenvalues:
%
\be
\hat{\mat{C}} = \frac{1}{M}\sum\limits_{i=1}^{M}\bm{d}_i\bm{d}_i^\top = \hat{\bm{W}}\hat{\bm{\Lambda}}\hat{\bm{W}}^\top.
\ee
%
At each subsequent iteration, a new set of Monte Carlo samples is generated using the joint 
probability law, $\pi_{\vec{\xi}}$. Since model evaluations are expensive, the number of new samples
is a factor of the initial number, $n_0$. The augmented set of available and newly generated model
evaluations are used to obtain improved estimates of $\hat{\mat{C}}$ using Algorithm~\ref{alg:lla}.
The eigenvalue decomposition of $\hat{\mat{C}}$ yields refined estimates of the eigenspace.
The improved eigenspace is partitioned as discussed earlier in~\ref{sub:as}
 to obtain the active subspace. For a given eigenvector ($j^{th}$) in the active subspace, 
 we compute the relative L-2 
 norm of the difference in squared values of corresponding components ($\varepsilon_j^k$) of the same eigenvector,
 computed during successive iterations (iterations $k$ and $k-1$) as follows:
%
\be
\varepsilon_j^k = \frac{\|(\hat{\mat{W}}_{1,j}^{k})^2 - 
                       (\hat{\mat{W}}_{1,j}^{k-1})^2\|_2}{\|(\hat{\mat{W}}_{1,j}^{k-1})^2\|_2}, 
                       j = 1,\ldots,p,
\label{eq:conv}
\ee
%
where $p$ denotes the dimension of the column space of $\mat{W}_1$ or the number of eigenvectors in
the active subspace.
The quantity, $\varepsilon_j^k$ is recorded as the $j^{th}$ component of $\vec\varepsilon^k$. Convergence is
established once the component with the highest magnitude is below a given tolerance, $\tau$. 
The sequence of steps for computing the active subspace is outlined
in Algorithm~\ref{alg:free}.
%
\bigskip
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{An iterative gradient-based approach for discovering the active subspace}
  \begin{algorithmic}[1]
\Require $\tau$
\Ensure $\hat{\mat{\Lambda}}$, $\hat{\mat{W}}$, $\bm{\nu}_p(G)$ %$\eta$. %
    \Procedure{Gradient-free}{}
    \State Set $k$ = 0
	\State Draw $n_k$ random samples, $\{\bm{\xi}_i\}_{i=1}^{n_k}$ 
         according to $\pi_{\bm{\xi}}$. 
    \State Set $N_\text{total}$ = $n_k$ 
	\State For each $i=1, \ldots, N_\text{total}$, compute
		\[
		\begin{aligned}
	          &{\color{white}=} G(\bm{\xi}_i)~\text{and}~\bm{g}^i = \nabla_{\bm{\xi}}G(\bm{\xi}_i)~
		 \text{using~Algorithm~\ref{alg:lla}}
		\end{aligned}
		\]
	\State Compute $\hat{\mat{C}}$ and its eigenvalue decomposition:
		\[
		\begin{aligned} 
		  \hat{\mat{C}} &= \frac{1}{N_\text{total}}\sum\limits_{i=1}^{N_\text{total}}[\bm{g}^i][\bm{g}^i]^\top 
		  &= \hat{\mat{W}}^{(k)}\hat{\mat{\Lambda}}^{(k)} \hat{\mat{W}}^{(k)\top}
		\end{aligned}
		\]
	\State Partition the eigenpairs:
	\[
		\begin{aligned}
		 \hat{\mat{\Lambda}}^{(k)}=
        	\begin{bmatrix} \hat{\mat{\Lambda}}_1^{(k)} & \\ & \hat{\mat{\Lambda}}_2^{(k)} \end{bmatrix}, 
        	\hat{\mat{W}}^{(k)}=\begin{bmatrix} \hat{\mat{W}}_1^{(k)} & \hat{\mat{W}}_2^{(k)} \end{bmatrix}, 
        	\hat{\mat{\Lambda}}_1^{(k)}\in \mathbb{R}^{N_p\times p}
		\end{aligned}
	\]
	\Loop
		\State Set $k$ = $k$ + 1
		\State Draw $n_k =  \lceil\beta n_{k-1}\rceil$  new samples 
                $\{\bm{\xi}_i\}_{i=1}^{n_k}$  $\beta\in[0,1]$
                
%	%	\State Project $\bm{\xi}_k$~$\rightarrow$~$\bm{\theta}_k$.%
		\State Set $N_\text{total}$ = $N_\text{total}$ + $n_k$ 
		\State Compute $\bm{g}^i = \nabla_{\bm{\xi}_i}G(\bm{\xi}_i)$, 
             	$i=n_{k-1}+1, \ldots, n_{k-1}+n_k$ using Algorithm~\ref{alg:lla} 
		\State Compute $\hat{\mat{C}}$ and its eigenvalue decomposition (see Step 6)
		\State Partition the eigenspace of $\hat{\mat{C}}$ as shown in Step 7 
		\State Compute, $\vec\varepsilon^k$ using~\eqref{eq:conv} 
		\If {$\max\left(\vec\varepsilon^k\right)<\tau$}
			\State break
		\EndIf
       \EndLoop
%	\State Compute the normalized activity scores, $\tilde{\nu}_{i,p}(G)$ using~\eqref{eq:ac} and~\eqref{eq:nac}
    \EndProcedure
  \end{algorithmic}
  \label{alg:free}
\end{breakablealgorithm}
\bigskip
%

\subsection{Surrogate construction}
\label{sub:surr_sub}

Computational savings as a result of dimension reduction can be enhanced by
constructing a surrogate, $\tilde{\mathcal{Y}}(\vec{\eta})$ to approximate $\mathcal{Y}(\vec{\eta})$
in the active subspace. For this purpose, the following sequence of steps
provided in~\cite{Constantine:2015} (chapter 4), and outlined below in Algorithm~\ref{alg:surr}
can be used.
\bigskip
\begin{breakablealgorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{For constructing the surrogate model, $\tilde{\mathcal{Y}}(\mat{W}_1^\top\vec\xi)$}
  \begin{algorithmic}[1]
	\Procedure{Surrogate Model, $\tilde{\mathcal{Y}}$}{} 
	  \State Consider $N$ available data points in the full space, $(\vec\xi_i,G(\vec\xi_i))$, $i~=~1,\ldots,N$
	  \State For each $\vec\xi_i$, compute $\vec\eta_i$ = $\mat{W}_1^\top\vec\xi_i$ 
          (Note: $\mathcal{Y}(\vec{\eta}_i)$ $\approx$ $G(\vec{\xi}_i)$)
	  \State Fit a regression surface, $\tilde{\mathcal{Y}}$ to approximate $\mathcal{Y}$ using the data
                 points, $(\vec\xi_i,\mathcal{Y}(\vec\eta_i))$
	  \State Note that the overall approximation is: $G(\vec{\xi})$ $\approx$
                 $\tilde{\mathcal{Y}}(\mat{W}_1^\top\vec{\xi})$ 
	\EndProcedure
  \end{algorithmic}
  \label{alg:surr}
\end{breakablealgorithm}
\bigskip

A 1-dimensional surrogate in Algorithm~\ref{alg:surr} can be easily accomplished by using a polynomial fit. 
In case the active subspace is higher dimensional, 
common approaches for surrogate modeling such as Gaussian Process 
(GP)~\cite{Rasmussen:2004}, and 
polynomial chaos ~\cite{Ghanem:1990, Xiu:2002} can be used. However, training the surrogate
in both cases can be expensive since it requires model runs or simulations. For high-dimensional
problems, training a surrogate can also be prohibitive if simulations are intensive. Dimension
reduction as a result of the proposed methodology would help reduce the number of training runs needed
to construct the surrogate. Moreover, in the forward problem, a large number of samples are typically
required to characterize the variability in the QoI due to input uncertainty. The computational
effort associated with forward uncertainty propagation is significantly reduced in the active subspace,
especially in the case of a GP surrogate\footnote{A GP surrogate carries the training data with it, in
order to calculate the covariance matrices w.r.t. the prediction point, whereas regression models such as
polynomial chaos only carry the coefficients derived from the training data; as a result, the computational
savings in the forward problem due to active subspace is much greater in the case of a GP surrogate
compared to a polynomial chaos surrogate.}. Furthermore, the surrogate can be exploited to perform GSA
as discussed below in~\ref{sub:scores}. 

\subsection{GSA using the Active Subspace}
\label{sub:scores} 

The resulting active subspace could be used to perform a GSA of the uncertain inputs i.e. the SW potential
parameters in two ways: (1) by estimating the total Sobol' index for each SW parameter, and (2) by computing
the so-called activity scores. Both approaches are expected to yield consistent trends for the parametric 
sensitivities, and are discussed as follows. 

\subsubsection{Surrogate-based GSA}
\label{subsub:gsa_surr}

Consider a model output, $G= G(\xi_1,\xi_2,\ldots,\xi_N)$, where $\xi_i$'s are statistically independent uncertain
inputs to the model. The total effect Sobol' index, $T_i(G)$, is a commonly used 
variance-based measure for global sensitivity 
analysis (GSA). For a given uncertain input, it combines both the individual contribution of an uncertain input and the
contribution due to its interaction with other uncertain inputs, to the variability in the output, $G$. Mathematically, it is 
expressed as follows:
%
\be
T_i(G) = 1 - 
\frac{\V[\mathbb{E}(G|\vec{\xi}_{\sim i})]}{\V(G)},
\label{eq:total}
\ee
%
where $\vec{\xi}_{\sim i}$ is a vector of uncertain inputs with the  $i^\text{th}$ entry removed, and $\V$ denotes the 
variance. The RHS in~\eqref{eq:total} involves multidimensional integrals and is typically estimated using numerical
techniques such as Monte Carlo sampling. Hence, obtaining converged estimates of $T_i(G)$ might require a large 
number of model runs depending upon the nature of the dependence of $G$ on $\xi_i$'s. To overcome this challenge,
we exploit the surrogate, $\tilde{\mathcal{Y}}$ (see Algorithm~\ref{alg:surr} and related discussion in~\ref{sub:surr_sub})
that essentially maps the canonical random variables, $\vec{\xi}$ to the QoI i.e the bulk thermal conductivity as 
$\tilde{\mathcal{Y}}(\mat{W}_1^\top\vec{\xi})$. Note that $\vec{\xi}$ exhibits a linear
relationship with the physical parameters, $\vec{\theta}$ as discussed earlier in~\ref{sub:as}.
Thus, a large number of
independent random samples in the physical input domain can be mapped to the QoI to compute $T_i(G)$
with negligible effort. 

\subsubsection{Activity Scores for GSA}
\label{subsub:gsa_as}
Several recent efforts have
focused on an efficient computation of the first-order and total effect Sobol' indices
using polynomial chaos surrogates~\cite{Sudret:2008}, density-based sensitivity measures~\cite{Plischke:2013},
 randomized orthogonal arrays~\cite{Tissot:2015}, and direct computation from the input-output 
 samples~\cite{Li:2016}. Additionally,
derivative-based global sensitivity measures (DGSMs)~\cite{Sobol:2009, Lamboni:2013}
have been developed to estimate upper bounds on the total effect Sobol' indices using a fraction of computational effort
otherwise required to estimate the indices themselves. It was shown 
in~\cite{Diaz:2016,Constantine:2017} (and later generalized for a broad range of input probability distributions 
in~\cite{Vohra:2018c}) that the eigenspace that constitutes the active subspace can be used to
approximate the DGSMs by evaluating the so-called activity scores. The activity score, $\nu_i$ for the 
$i^{th}$ uncertain input can be computed using the following expression:
%
\be
\nu_{i,p}(G) = \sum\limits_{j=1}^{p} \lambda_j w_{i,j}^2, i=1,\ldots,\Nt,
\label{eq:ac}
\ee
%
where $p$ denotes the dimensionality of the eigenspace. 
A more robust measure is the normalized activity score ($\tilde{\nu}_{i,p}(G)$) that ranges from 0 to 1:
%
\be
\tilde{\nu}_{i,p}(G) = \scalebox{1.25}{$\frac{\nu_{i,p}(G)}{\sum_i\nu_{i,p}(G)}$}.
\label{eq:nac}
\ee
%
The normalized activity scores are computed for the SW parameters to determine their relative contributions
to bulk thermal conductivity estimates of Si and compared with corresponding estimates using the surrogate-based
approach, in section~\ref{sec:results}. 

\subsection{Bayesian calibration using the Active Subspace}
\label{sub:ba_method}

The Bayesian framework provides a robust machinery for calibrating unmeasured inputs to 
or unknown parameters of a computational model or
a simulation in the presence of uncertainty. The framework allows incorporation of multiple
sources of uncertainty in a systematic manner. These sources include prior knowledge
(including expert opinion)
 pertaining to the inputs or parameters to be calibrated, output measurement errors, potential noise inherent
in simulations, and numerical errors and model form errors in the computational model.
 The objective is to evaluate the joint
conditional probability distribution of the uncertain inputs or parameters to be calibrated,
given a set of observations and the
associated experimental and computational uncertainties. This distribution is referred to as
the joint posterior of the inputs/parameters, and is evaluated using the Bayes' theorem:
%
\be
\mathbb{P}(\bm{\theta}\vert \bm{D}) = 
\frac{\mathbb{P}(\bm{D}\vert\bm{\theta})}{\mathbb{P}(\vec{D})}\mathbb{P}(\bm{\theta}),
\ee
%
where $\vec{D}$ is the available set of observations. 
The denominator in the RHS, $\mathbb{P}(\vec{D})$ is the evidence term i.e. the product of the likelihood, 
$\mathbb{P}(\bm{D}\vert\bm{\theta})$ and the joint prior distribution,
$\mathbb{P}(\bm{\theta})$, integrated over all possible values of the inputs to be calibrated, $\vec{\theta}$. 
Hence, $\mathbb{P}(\vec{D})$ is a proportionality term and in practice the joint posterior, 
$\mathbb{P}(\bm{\theta}\vert \bm{D})$ is usually evaluated up to a proportionality constant as follows:
%
\be
\mathbb{P}(\bm{\theta}\vert \bm{D}) \propto
\mathbb{P}(\bm{D}\vert\bm{\theta})\mathbb{P}(\bm{\theta}).
\label{eq:bayes}
\ee
%
Several Markov Chain Monte Carlo-based algorithms are available to sample the joint posterior using the
relationship in~\eqref{eq:bayes}~\cite{Haario:2001, Haario:2006,Xu:2014}.
However, they typically require tens of thousands of model runs which would be
impractical for the present application. The active subspace can be used to mitigate the
computational effort in two ways: Firstly, as a result of GSA, we can identify important parameters based on
their contributions to the variability in the QoI. The process of Bayesian parameter estimation
can thus be expedited signficantly by focusing on calibrating only the important parameters while fixing other
parameters at their nominal values. Secondly, the active subspace-based low-dimensional
surrogate can be used in lieu of the model in order to reduce the computational effort associated with the
likelihood estimation. In Section~\ref{sec:results}, we implement the proposed methodology to calibrate the
important SW parameters in a Bayesian setting. 




